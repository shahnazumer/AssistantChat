{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neccessary imports\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AssistantChat\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# creating vectorestore \"Alice In Wonderland Story\" - dont mind its my fav one \n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"Alice, a young girl, follows a rabbit down a rabbit hole and finds herself in a fantastical world.\",\n",
    "    \"She encounters strange creatures, including the Cheshire Cat, the Mad Hatter, and the Queen of Hearts.\",\n",
    "    \"The world she enters is nonsensical, with bizarre events and characters defying logic.\",\n",
    "    \"Alice's size constantly changes, leading to amusing and challenging situations.\",\n",
    "     \"The Cheshire Cat appears and disappears at will, offering cryptic advice.\",\n",
    "     \"The Mad Hatter and the March Hare are stuck in perpetual tea time, engaging Alice in eccentric conversations.\",\n",
    "     \"The Queen of Hearts is known for her love of croquet and her famous catchphrase, Off with their heads!\",\n",
    "     \"Alice attends a trial where nonsensical accusations are made, and the Queen orders executions.\",\n",
    "     \"Alice realizes the absurdity of Wonderland and challenges the Queen's authority.\",\n",
    "     \"She eventually wakes up, realizing it was all a dream, leaving Wonderland's eccentric characters behind.\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice was a young girl who followed a rabbit down a rabbit hole and found herself in a fantastical world.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"who was alice?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer in the following language: {language}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Alice decides to leave Wonderland at the end of the story because she realizes the absurdity of Wonderland and challenges the Queen's authority. Additionally, she eventually wakes up, realizing it was all a dream, and leaves behind Wonderland's eccentric characters.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Why does Alice decide to leave Wonderland at the end of the story?\", \"language\": \"english\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating app with qdrant - this time vectorestore ill be adding about ALbert Einsten, ask QnA about his work\n",
    "\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import qdrant_client\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create qdrant client\n",
    "\n",
    "os.environ['QDRANT_HOST']\n",
    "os.environ[\"QDRANT_API_KEY\"]\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    os.getenv(\"QDRANT_HOST\"),\n",
    "    api_key=os.getenv(\"QDRANT_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file\n",
    "file=\"my.txt\"\n",
    "data=\"\"\n",
    "\n",
    "with open(file,'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the text into chunks\n",
    "#create a function to return chunks\n",
    "def get_chunks(text):\n",
    "    text_splitter=CharacterTextSplitter(\n",
    "        separator= \"\\n\",\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100, # second chunk start  character from 800, overlap is used to stop loosing chunk \n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    chunks=text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the chunks for the data\n",
    "texts=get_chunks(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2530"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a new collection\n",
    "\n",
    "vectors_config=models.VectorParams(\n",
    "    # depends on model, we can google dimension. 1536 for openai\n",
    "    # we are using openai embedding, for that size is 1536\n",
    "    size=1536, \n",
    "    distance=models.Distance.COSINE)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"Albert Einsten\",\n",
    "    vectors_config=vectors_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector store object using langchain\n",
    "\n",
    "# if we want to use any other embedding, we need to change size\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "vector_store = Qdrant(\n",
    "    client=client, \n",
    "    collection_name=\"Albert Einsten\", \n",
    "    embeddings=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fe8af872e7ce4c47a3dd445e391f6ad9',\n",
       " 'a86b604f91b441b8872b4ffbc7ecadd0',\n",
       " '31f704b4baf54759874c88e4eccb616b',\n",
       " '77267bb5173c47be9ff0a320892b7767',\n",
       " 'faa899f6706c44f1b3c047452c26bbfa',\n",
       " 'bb174938e99b436e89d7659926e73490',\n",
       " '3186faed36984934b0393c2d2e841794']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add chunks to vector store\n",
    "vector_store.add_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Qdrant', 'OpenAIEmbeddings'] vectorstore=<langchain_community.vectorstores.qdrant.Qdrant object at 0x000001B1C3126910>\n"
     ]
    }
   ],
   "source": [
    "retriever=vector_store.as_retriever()\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plug vector store into retrieval chain\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import retriever\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(),\n",
    "    chain_type=\"stuff\",  # Positional argument before keyword argument\n",
    "    retriever=vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Einstein's theory is the general theory of relativity, which incorporates the force of gravity and provides a new understanding of how massive objects influence the fabric of spacetime. \n"
     ]
    }
   ],
   "source": [
    "query = \"What is Einsten theory?\"\n",
    "response = qa.run(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
